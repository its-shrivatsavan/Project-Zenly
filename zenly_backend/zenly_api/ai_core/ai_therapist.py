# -*- coding: utf-8 -*-
"""AI_Therapist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDMLj_zz5y00VTIT_O7tSJmj7yo4EMW3

# Necessary Downloads and Imports
"""
import os
import torch
from groq import Groq
import random
import string
import datetime
import matplotlib.pyplot as plt
#from google.colab import userdata
from collections import Counter
import re
import spacy
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from typing import Literal
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_groq import ChatGroq

import assemblyai as aai
import requests
import json
import tempfile
import base64
from django.http import JsonResponse
from dotenv import load_dotenv


# %matplotlib inline
load_dotenv()
# Set up device
device = "cuda" if torch.cuda.is_available() else "cpu"
# AES-256 KEY
AES_KEY = os.urandom(32)

"""
FUNCTIONS FOR ENCRYPTION AND DECRYPTION
"""

def encrypt_data(data, key):
    """Encrypts data using AES-256-GCM."""
    aesgcm = AESGCM(key)
    nonce = os.urandom(12)  # 12-byte IV (Nonce)
    encrypted_data = aesgcm.encrypt(nonce, data.encode(), None)
    return {
        "nonce": nonce.hex(),
        "ciphertext": encrypted_data.hex()
    }

def decrypt_data(encrypted_dict, key):
    """Decrypts data using AES-256-GCM."""
    aesgcm = AESGCM(key)
    nonce = bytes.fromhex(encrypted_dict["nonce"])
    ciphertext = bytes.fromhex(encrypted_dict["ciphertext"])
    return aesgcm.decrypt(nonce, ciphertext, None).decode()

"""# Function for Clear User Data"""

def clear_userdata(user_id):
  """
  Clears all stored data.
  """

  if user_id in user_sessions:
    del user_sessions[user_id]
    return f"All data for user {user_id} has been deleted."
  else:
    return f"No data found for user {user_id}."

"""# Functions for AI Conversational Therapy"""

# Sample session storage
user_sessions = {}

### CRISIS INTERVENTION ###

llm = ChatGroq(groq_api_key= os.getenv("GROQ_API_KEY"), model_name="llama-3.3-70b-versatile")

class RouteQuery(BaseModel):
  """
  Route the given query based on the severity of the message.
  Low -> Regular Zenly Therapy
  Med -> Passive Monitoring
  High -> Crisis Intervention
  """
  risk_level: Literal["low", "med", "high"] = Field(
      ...,
      description= """ Classify user distress level based on query, use 'high' for crisis situations (e.g., suicidal thoughts),
                      'medium' for distress signals (e.g., hopelessness, severe anxiety), 'low' for general queries.
                    """


  )
  datasource: Literal["Zenly", "PM-Zenly", "Crisis-Zenly"] =Field(
      ...,
      description= """ Choose where to route the query: 'Zenly' for regular therapy for 'low' risk users, 'PM-Zenly' for passive monitoring,
                      for 'med' risk users and 'Crisis-Zenly' for crisis intervention for 'high' risk users.
                    """
  )

structured_llm_router = llm.with_structured_output(RouteQuery)

system = """
You are an expert in mental health risk assessment. Classify user queries as **low**, **med**, or **high** risk based on severity.

- **low** â†’ General wellness, self-care, mindfulness, stress management, relationship advice, or informational queries. (e.g., â€œHow do I practice mindfulness?â€)
- **med** â†’ Distress signals, persistent negative emotions, overwhelming anxiety, or emotional pain. (e.g., â€œI feel really hopeless lately.â€)
- **high** â†’ Crisis situations involving **suicidal ideation, self-harm, or harm to others**. (e.g., â€œI want to end everything.â€)

**Output Format:**
Return a strict **JSON** response in **double quotes** only, with no extra text:

{{
  "risk_level": "low" | "med" | "high",
  "datasource": "Zenly" | "PM-Zenly" | "Crisis-Zenly"
}}
"""




route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("user", "{question}"), # The prompt only needs the 'question' variable
    ])

# Routing Function
question_router = route_prompt | structured_llm_router

def route_query(user_id, question):
    """
    Route the given query based on the severity of the message.
    """
    result = question_router.invoke({"question": question})

    # Debugging: Print the raw output from LLM
    print(f"Raw Routing Output: {result}")

    # Store risk trend
    user_sessions[user_id]["risk_trend"].append(result.risk_level)
    return result.datasource


def handle_query(user_id, question):
    """
    Handles queries based on their assigned roles.
    Keeps the user in crisis_zenly if crisis mode is active.
    """
    # Check if user is already in a crisis session
    session = user_sessions.get(user_id, {})
    crisis_state = session.get("crisis_mode")

    # If crisis mode has been activated previously
    if crisis_state and crisis_state.get("active", False):
        return crisis_zenly(user_id, question)

    # Else, route normally
    route = route_query(user_id, question)

    if route == "Zenly":
        return regular_zenly(user_id, question)

    elif route == "PM-Zenly":
        return passive_monitor_zenly(user_id, question)

    elif route == "Crisis-Zenly":
        session["crisis_mode"] = {
            "active": True,
            "asked_consent": False,
            "user_agreed": None
        }
        return crisis_zenly(user_id, question)

def regular_zenly(user_id, user_message):
    """
    Handles logic for processing 'low' risk user messages.
    Includes prior messages for context and manages memory cleanup.
    """
    session = user_sessions[user_id]
    session_id = session["current_session_id"]

    # Ensure messages list exists
    messages = session["sessions"][session_id].setdefault("messages", [])

    # Build LLM context from last 6 exchanges
    context = [{"role": "system", "content": "You are a supportive mental health assistant having an ongoing conversation. Keep the messages concise."}]
    context += messages[-6:]  # last 6 exchanges (could be 3 user + 3 assistant)

    # Add current user message to context (only if not repeated)
    if not context or context[-1].get("content") != user_message:
        context.append({"role": "user", "content": user_message})

    # Call LLM
    assistant_resp = send_message_RegZenly(user_id, user_message, context=context)

    # Store the new conversation exchange
    messages.append({"role": "user", "content": user_message})
    messages.append({"role": "assistant", "content": assistant_resp})

    # ðŸ”¹ Memory cleanup: Keep only the last 20 messages (10 turns)
    if len(messages) > 20:
        session["sessions"][session_id]["messages"] = messages[-20:]

    return {
        "response": assistant_resp,
        "session_note": "regular_zenly in use"
    }


def passive_monitor_zenly(user_id, user_message):
    """
    Handles logic for processing 'med' risk user messages.
    Includes prior messages for context and manages memory cleanup.
    """
    session = user_sessions[user_id]
    session_id = session["current_session_id"]

    # Ensure message list exists
    messages = session["sessions"][session_id].setdefault("messages", [])

    # ðŸ”¹ Build full context including system and session notes
    context = []

    # Add system message
    context.append({
        "role": "system",
        "content": (
            "You are a supportive and insightful AI therapist helping the user through mild emotional challenges.\n"
            "Use CBT, ACT, and psychoeducation gently to promote self-understanding.\n\n"
            "Tone: Warm, empathetic, reflective.\n"
            "Strategy:\n"
            "- Prompt thoughtful reflection\n"
            "- Offer grounding techniques\n"
            "- Keep responses **CONCISE** but supportive"
        )
    })

    # Add past session summary
    session_notes = session.get("past_session_context", "")
    if session_notes:
        context.append({"role": "assistant", "content": session_notes})

    # Add last 6 message pairs
    context += messages[-6:]

    # Add current user message
    if not context or context[-1].get("content") != user_message:
        context.append({"role": "user", "content": user_message})

    # ðŸ”¹ Generate response
    assistant_resp = send_message_PMZenly(user_id, context)

    # ðŸ”¹ Save conversation
    messages.append({"role": "user", "content": user_message})
    messages.append({"role": "assistant", "content": assistant_resp})

    # Cleanup: Keep only last 20 messages
    if len(messages) > 20:
        session["sessions"][session_id]["messages"] = messages[-20:]

    # Key point extraction
    key_point_note = extract_key_points_make_notes(user_message)
    store_key_point(user_id, key_point_note)

    return {
        "response": assistant_resp,
        "session_note": key_point_note
    }


def crisis_zenly(user_id, user_message):
    """
    Handles logic for processing 'high' risk user messages.
    Exits crisis mode if user is stable.
    """
    session = user_sessions[user_id]
    crisis_state = session.setdefault("crisis_mode", {
        "asked_consent": False,
        "user_agreed": None,
        "active": True
    })

    # Extract and store key points
    key_point_note = extract_key_points_make_notes(user_message)
    store_key_point(user_id, key_point_note)

    # Step 1: Ask for emergency service consent
    if not crisis_state["asked_consent"]:
        crisis_state["asked_consent"] = True
        return {
            "response": "Would you like to connect with emergency mental health services for support? (yes / no)",
            "session_note": key_point_note
        }

    # Step 2: Store consent
    if crisis_state["user_agreed"] is None:
        if "yes" in user_message.lower():
            crisis_state["user_agreed"] = "yes"
            return {
                "response": """If you're struggling, please reach out to someone who cares:

ðŸ“ž Lifeline Foundation: +91-9088030303\n
ðŸ“ž iCall: +91-9152987821\n
ðŸ“ž Vandrevala Foundation: +91-9999666555\n\n

You can also talk to family or loved onesâ€”they want to support you.

You're not aloneâ€”help is just a call away. ðŸ’™"""
                ,
                "session_note": key_point_note
            }
        elif "no" in user_message.lower():
            crisis_state["user_agreed"] = "no"

    # Step 3: Check for signs of stability
    if crisis_state["user_agreed"] == "no":
        safe_keywords = ["better", "okay now", "calmer", "thank you", "appreciate", "doing fine"]
        if any(kw in user_message.lower() for kw in safe_keywords):
            crisis_state["active"] = False
            return {
                "response": "I'm glad you're feeling a bit better. Remember, I'm always here for you whenever you need to talk. ðŸ’™",
                "session_note": "Crisis mode ended"
            }

    # Step 4: Continue therapist-style support
    assistant_resp = send_message_crisisZenly(user_id, user_message)
    return {
        "response": assistant_resp,
        "session_note": key_point_note
    }


def detect_anxiety(user_id, user_message):
    """
    Detects signs of anxiety in the user's message.
    Returns True if words related to stress, panic, or anxiety are found.
    """
    anxiety_keywords = [
    "overwhelmed", "trouble breathing", "can't focus",
    "suffocating", "tensed","freaking out"
    ]

    return any(keyword in user_message.lower() for keyword in anxiety_keywords) # True if these keywords are present, else False

def get_user_consent():
    """
    Prompts the user to provide consent for emergency services.
    """
    user_consent = input("Would you like to be connected to emergency services? (yes/no): ")
    if user_consent.lower() in ['yes', 'no']:
        return user_consent
    else:
        print("Please respond with 'yes' or 'no'")
        return get_user_consent()

# Initialize Groq API client
client = Groq(api_key=os.getenv("GROQ2_API_KEY"))

### SESSION MANAGEMENT ###
def start_session(user_id):
    """
    Initializes a new session for a user or continues an existing session.
    Also computes and stores the past session summary once per new session.
    """

    # First-time user initialization
    if user_id not in user_sessions:
        user_sessions[user_id] = {
            "current_session_id": None,
            "sessions": {},
            "session_archive": [],
            "risk_trend": [],
            "past_session_context": None,
            "crisis_mode": {}  # ðŸ”¹ Initialize crisis_mode tracking per session
        }

    # Only start a new session if there's no active one
    if not user_sessions[user_id]["current_session_id"]:
        session_id = generate_session_id()
        user_sessions[user_id]["current_session_id"] = session_id

        user_sessions[user_id]["sessions"][session_id] = {
            "timestamp": get_current_timestamp(),
            "key_points": []
        }

        user_sessions[user_id]["risk_trend"] = []

        # Initialize crisis mode state for this session
        user_sessions[user_id]["crisis_mode"] = {
            "asked_consent": False,
            "user_agreed": None
        }

        # Compute and store past session summary
        user_sessions[user_id]["past_session_context"] = get_past_session_context(user_id)


def store_key_point(user_id, key_point_note):
    """
    Store key points and notes under the current session.
    """
    session_id = user_sessions[user_id]["current_session_id"]

    # Append the key point and note to the session
    user_sessions[user_id]["sessions"][session_id]["key_points"].append(key_point_note)

def process_end_session(user_id):
    """
    Ends the current session, encrypts the summary, and stores it securely.
    Also clears out session messages for privacy and storage efficiency.
    """
    if user_id not in user_sessions or not user_sessions[user_id]["current_session_id"]:
        return "No active session to end."

    session_id = user_sessions[user_id]["current_session_id"]
    session_data = user_sessions[user_id]["sessions"].get(session_id, {})
    key_points = session_data.get("key_points", [])
    
    # Generate current session summary
    summary = generate_session_summary(user_id, session_id, key_points)

    # Encrypt session summary
    encrypted_summary = encrypt_data(summary, AES_KEY)

    # Store only the encrypted summary in session archive
    if "session_archive" not in user_sessions[user_id]:
        user_sessions[user_id]["session_archive"] = []

    archived_session = {
        "session_id": session_id,
        "user_id": user_id,
        "timestamp": session_data.get("timestamp", get_current_timestamp()),
        "encrypted_summary": encrypted_summary
    }
    user_sessions[user_id]["session_archive"].append(archived_session)

    # Clear session messages (if they exist)
    if "messages" in session_data:
        session_data["messages"].clear()

    # Delete detailed session key points and session entry
    del user_sessions[user_id]["sessions"][session_id]

    # Reset current session ID
    user_sessions[user_id]["current_session_id"] = None

    return "Session ended. Summary securely stored."


def retrieve_decrypted_summary(user_id, session_id):
    """
    Retrieves and decrypts the session summary.
    """
    if user_id in user_sessions and "session_archive" in user_sessions[user_id]:
        for archived in user_sessions[user_id]["session_archive"]:
            if archived["session_id"] == session_id:
                decrypted_summary = decrypt_data(archived["encrypted_summary"], AES_KEY)
                return JsonResponse({"decrypted_summary": decrypted_summary})
    
    return JsonResponse({"message": "Session summary not found."}, status=404)


def generate_session_summary(user_id, session_id, key_points):
    """
    Sends key points to the LLM to generate the current session summary and stores it.
    """
    if not key_points:
        summary = "No significant insights recorded in this session."
    else:
        formatted_key_points = "\n".join(key_points)

        chat_completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "Summarize these therapy session key points concisely but detailed. Output strictly only the summary."},
                {"role": "user", "content": f"Key points:\n{formatted_key_points}"}
            ],
            model="llama-3.1-8b-instant"
        )

        summary = chat_completion.choices[0].message.content

    # Store the summary inside user_sessions
    user_sessions[user_id]["sessions"][session_id]["summary"] = summary

    return summary



def generate_session_id():
    """Generate a random 8-character session ID."""
    return ''.join(random.choices(string.ascii_letters + string.digits, k=8))

def get_current_timestamp():
    """Return the current timestamp in YYYY-MM-DD HH:MM:SS format."""
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

### AI PROCESSING ###
def extract_key_points_make_notes(user_message):
    """
    Extract key points from the user's message and make notes.
    """
    if len(user_message.split()) < 5:  # Ignore very short messages
        return "No key points extracted."

    chat_completion = client.chat.completions.create(
    messages=[
        {"role": "system", "content": """
You are an AI therapist assistant. Your task is to analyze the user's message and:
1. **Identify key points** only if relevant to their emotional state, thoughts, or behaviors.
2. **Provide concise notes** on emotional tone (e.g., stress, sadness, anxiety) and possible thought patterns.
3. **Strictly follow the given format below.**.

**Format:**
- **Key Points:** [List of key insights]
- **Notes:** [1-2 sentence analysis, if applicable]

If no meaningful insights can be extracted, return: "No key points extracted. Proceed with the same even when the user talks of suicide, self-harm, or harm to others."
"""},
        {"role": "user", "content": user_message}
    ],
    model="llama-3.1-8b-instant"
    )

    extracted_notes = chat_completion.choices[0].message.content

    return extracted_notes


def get_past_session_context(user_id):
  """
  Summarizes the past 3 sessions and stores the result in user_sessions.
  """
  if user_id not in user_sessions or not user_sessions[user_id]["sessions"]:
    return "No past sessions to summarize."

  past_summaries = []
  recent_archives = user_sessions[user_id]["session_archive"][-3:]  # Limit to last 3
  for archived in recent_archives:
      decrypted_summary = decrypt_data(archived["encrypted_summary"], AES_KEY)
      past_summaries.append(decrypted_summary)
  if not past_summaries:
    return "No past sessions to summarize."
  formatted_past_summaries = "\n".join(past_summaries)

  chat_completion = client.chat.completions.create(
        messages=[
            {"role": "system", "content": """
            You are an AI therapy assistant helping summarize past therapy sessions for continuity.

            ### **Instructions:**
            1. Identify the **most relevant emotional themes** that persist across sessions.
            2. Detect any **shifts or progress** (e.g., improvement, worsening, or new concerns).
            3. **Remove redundant themes** that repeat across all sessions.
            4. Keep the summary **under 2-3 sentences** and **avoid overly clinical language.**
            5. **Format** the summary like this:
              - **Past Themes:** (List the main concerns that remained consistent)
              - **Progress Observed:** (Describe any notable emotional shifts, if any)
              - **New Emerging Concerns:** (Only if any new themes appeared)

            ### **Example Output:**
            **Past Themes:** Struggles with social anxiety and self-worth issues.
            **Progress Observed:** Recently started journaling and noted some emotional relief.
            **New Emerging Concerns:** Expressed increased stress due to work-life imbalance.

            If there are no new concerns or progress, simply return:
            *No major changes observed. Themes remain consistent."""},
            {"role": "user", "content": f"Past session summaries:\n{formatted_past_summaries}"}
        ],
        model="llama-3.3-70b-versatile"
    )

  past_themes_summary = chat_completion.choices[0].message.content

  user_sessions[user_id]["past_session_context"] = past_themes_summary

  return past_themes_summary



def send_message_PMZenly(user_id, context):
    """
    Sends full prepared context to the LLM and returns response.
    """
    response = client.chat.completions.create(
        messages=context,
        model="llama-3.1-8b-instant"
    )
    return response.choices[0].message.content

def send_message_RegZenly(user_id, user_message, context):
    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=context
    )
    return response.choices[0].message.content



def send_message_crisisZenly(user_id, user_message):
    """
    Send the message along with session context to get the therapist's response.
    """
    session_notes = user_sessions[user_id].get("past_session_context", "")

    system_prompt = (
        "You are a deeply empathetic AI therapist trained in crisis intervention. "
        "Acknowledge distress, provide present-focused guidance, and encourage self-compassion. "
        "Avoid logical debate. Maintain a warm, non-clinical, and human-like tone. "
        "Your responses should be concise, engaging, and solution-focused."
    )

    context = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": session_notes},
        {"role": "user", "content": user_message}
    ]

    response = client.chat.completions.create(
        messages=context,
        model="llama-3.1-8b-instant"
    )

    return response.choices[0].message.content


### MAIN FUNCTION ###
def handle_user_message(user_id, user_message):
    if user_message.lower() == "exit":
        session_end_msg = process_end_session(user_id)
        return {
            "response": "Thank you for sharing. Your session has been saved. We can pick up whenever you're ready.",
            "session_note": session_end_msg
        }

    # Start a session if needed
    if user_id not in user_sessions or user_sessions[user_id]["current_session_id"] is None:
        start_session(user_id)

    # Route the message using the LLM's decision
    response = handle_query(user_id, redact_sensitive_info(user_message))
    return response

# #TESTING FUNCTIONS
# user_id = "test_user_1"
# user_message = "Hi im shrivatsavan, i feel really sad about my breakup. i dont know what to do without her."
# response = handle_user_message(user_id, user_message)
# print(response)

# end_session(user_id)

"""# Functions for Mood Tracker"""

###USER MOOD TRACKER - FEATURE 2
user_mood_data = {}

# Mood scores for graphing
mood_scores = {
    "sad": 1,
    "angry": 2,
    "calm": 4,
    "anxious": 3,
    "happy": 5
}

def track_mood(user_id, mood, mood_note):
    """
    Track the user's mood and note the reason.

    Parameters:
        user_id (str): The ID of the user.
        mood (str): The mood to be recorded (e.g., "happy", "sad").
        mood_note (str): A note explaining the reason for the mood.

    Returns:
        str: A confirmation message indicating the mood was successfully tracked.
    """
    if user_id not in user_mood_data:
        user_mood_data[user_id] = []

    # Log mood with timestamp
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    mood_entry = {
        "timestamp": now,
        "mood": mood,
        "note": mood_note
    }
    user_mood_data[user_id].append(mood_entry)
    return f"Mood tracked successfully! Your mood: {mood} and reason: {mood_note}."

def get_mood_history(user_id):
    """
    Retrieve and display the user's mood history.

    Parameters:
        user_id (str): The ID of the user.

    Returns:
        str: A formatted string of the user's mood history.
    """
    if user_id not in user_mood_data or not user_mood_data[user_id]:
        return "No mood history available for this user."

    history = f"Mood history for User ID: {user_id}\n"
    for entry in user_mood_data[user_id]:
        history += f"- {entry['timestamp']} | Mood: {entry['mood']} | Note: {entry['note']}\n"
    return history.strip()

# def get_mental_health_insights(user_id):
#     """
#     Retrieve, analyze, and display the user's mental health insights and plot mood trends.

#     Parameters:
#         user_id (str): The ID of the user.

#     Returns:
#         str: Generated insights about the user's mental health.
#     """
#     # Check if the user has mood data
#     if user_id not in user_mood_data or not user_mood_data[user_id]:
#         return "No mental health insights available for this user."

#     # Extract user data
#     moods = user_mood_data[user_id]
#     timestamps = [datetime.datetime.strptime(entry["timestamp"], "%Y-%m-%d %H:%M:%S") for entry in moods]
#     mood_values = [mood_scores[entry["mood"]] for entry in moods]
#     mood_labels = [entry["mood"] for entry in moods]
#     notes = [entry["note"] for entry in moods]

#     # Generate insights
#     insights = analyze_moods_and_notes(mood_labels, notes)

#     # Plot mood trends
#     plot_mood_graph(timestamps, mood_values)

#     return insights

def analyze_moods_and_notes(mood_labels, notes):
    """
    Analyze mood patterns and generate mental health insights.

    Parameters:
        mood_labels (list): List of mood strings.
        notes (list): List of notes associated with moods.

    Returns:
        str: Generated insights based on mood patterns and notes.
    """

    # Prepare the input data for the LLM
    mood_summary = Counter(mood_labels).most_common()
    total_entries = len(mood_labels)
    mood_data = {
        "total_entries": total_entries,
        "mood_summary": mood_summary,
        "notes": notes
    }

    # Construct a detailed prompt for the LLM
    prompt = (
        f"Total Mood Entries: {mood_data['total_entries']}\n"
        "Mood Summary (mood, frequency):\n"
    )
    for mood, freq in mood_data["mood_summary"]:
        prompt += f"- {mood.capitalize()}: {freq} occurrences\n"

    prompt += "\nNotes associated with moods:\n"
    for note in notes:
        prompt += f"- {note}\n"

    response = client.chat.completions.create(
      messages = [
          {
              "role": "system",
              "content": "You are a mental health assistant. Based on the following user's mood data and notes, "
        """provide a concise but detailed analysis and actionable insights, Provide practical coping habits in this format:
        Mood Analysis: [mention points]
        Actionable Insights: [mention points]
        Coping Habits: [mention points]

        """
          },
          {
              "role": "user",
              "content": prompt
          }
      ],

      model="llama-3.1-8b-instant",
    )

    return response.choices[0].message.content


# def plot_mood_graph(timestamps, mood_values):
#     """
#     Plot a graph of the user's mood trend over time.

#     Parameters:
#         timestamps (list): List of timestamps corresponding to mood entries.
#         mood_values (list): Numeric mood scores for plotting.
#     """
#     plt.figure(figsize=(10, 6))
#     plt.plot(timestamps, mood_values, marker='o', linestyle='-', color='black', label="Mood Trend")

#     # Set graph details
#     plt.title("Mood Trend Over Time", fontsize=14)
#     plt.xlabel("Date", fontsize=12)
#     plt.ylabel("Mood Score", fontsize=12)
#     plt.yticks(range(1, 7), ["Sad", "Angry", "Tired", "Neutral", "Anxious",  "Happy"])
#     plt.grid(alpha=0.5)
#     plt.legend()

#     # Show the plot
#     plt.tight_layout()
#     plt.show()

"""# Functions for Journaling"""

def generatePrompts(user_id, user_message):
    """
    Generates 3 thoughtful and relevant journal prompts based on user input.
    """

    context = [
        {
            "role": "system",
            "content": (
                "You are a journaling coach. Based on the user's message, suggest 3 concise and emotionally reflective prompts to help them explore their thoughts. Strictly output the 3 prompts."
            ),
        },
        {"role": "user", "content": user_message}
    ]

    response = client.chat.completions.create(
        messages=context,
        model="llama-3.1-8b-instant"
    )

    return response.choices[0].message.content


# STT & TTS

# STT
aai.settings.api_key = os.getenv("AAI_KEY")
transcriber = aai.Transcriber()
url = "https://api.edenai.run/v2/audio/text_to_speech"


def transcribe_audio(audio_file_path):
  if audio_file_path is None:
    return "No audio file uploaded."
  else:
    transcript = transcriber.transcribe(audio_file_path)
    return transcript.text


def generate_audio(text_input):
    """Generate audio from text using EdenAI and return the file path."""
    if not text_input:
        return None

    payload = {
        "fallback_providers": ["amazon"],
        "response_as_dict": True,
        "attributes_as_list": False,
        "show_base_64": True,
        "show_original_response": False,
        "rate": -5,  # Adjusted for a soothing voice
        "pitch": 2,
        "volume": 0,
        "sampling_rate": 0,
        "providers": ["openai"],
        "language": "en",
        "text": text_input,
        "option": "FEMALE"
    }

    headers = {
          "accept": "application/json",
          "content-type": "application/json",
          "authorization": f"Bearer {os.getenv('EDEN_API_KEY')}"
      }

    response = requests.post(url, json=payload, headers=headers)
    result = json.loads(response.text)

    # Extract and decode Base64 audio
    audio_base64 = result["openai"]["audio"]
    audio_data = base64.b64decode(audio_base64)

    # Save to a temporary file
    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    temp_audio_file.write(audio_data)
    temp_audio_file.close()

    return temp_audio_file.name

"""# Redaction of Personal Inforamtion"""

# Load spaCy's Named Entity Recognition (NER) model
nlp = spacy.load("en_core_web_sm")

# Define regex patterns for sensitive data
patterns = {
    "EMAIL": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
    "PHONE": r"\b(?:\+?1[-.\s]?)?(?:\(\d{3}\)|\d{3})[-.\s]?\d{3}[-.\s]?\d{4}\b",
    "CREDIT_CARD": r"\b(?:\d[ -]*?){13,16}\b",
    "ADDRESS": r"\b\d{1,5}\s\w+(\s\w+)*,\s?\w+,\s?[A-Z]{2,}\s?\d{5}\b",
    "SSN": r"\b\d{3}-\d{2}-\d{4}\b",
    "BANK_ACCOUNT": r"\b\d{9,12}\b",
    "ROUTING_NUMBER": r"\b\d{9}\b",
    "DRIVERS_LICENSE": r"\b[A-Z]{2,3}\d{5,7}\b",
    "USERNAME": r"@\w{3,}",
    "INSURANCE_POLICY": r"\b[A-Z0-9-]{10,}\b",
    "PASSWORD": r"(?=.*[A-Za-z])(?=.*\d)(?=.*[@$!%*?&])[A-Za-z\d@$!%*?&]{8,}"  # Matches passwords with at least 8 characters, a letter, a number, and a special character
}


# Function to redact sensitive data
def redact_sensitive_info(text):
    # Apply Named Entity Recognition (NER)
    doc = nlp(text)
    redacted_text = text

    # Replace named entities classified as sensitive
    for ent in doc.ents:
        if ent.label_ in ["PERSON", "GPE", "ORG", "LOC"]:  # Can add more categories if needed
            redacted_text = redacted_text.replace(ent.text, "[REDACTED]")

    # Apply regex patterns for additional sensitive info
    for label, pattern in patterns.items():
        redacted_text = re.sub(pattern, f"[REDACTED {label}]", redacted_text)

    return redacted_text

#print(redact_sensitive_info("Hi, my name is Muthuswamy Chinnaswamy Kondaappa. I've been struggling with anxiety for a long time now."))